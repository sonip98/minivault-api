# ðŸ§  MiniVault API

> **ModelVault Take-Home Project** â€” A local-first developer tool that runs LLMs completely offline.

---

## ðŸš€ Overview

MiniVault API is a lightweight, local REST API that simulates a core ModelVault feature: **running an LLM locally to respond to user prompts** â€” entirely offline.

This project uses:

* ðŸŸ¢ Node.js with [Fastify](https://fastify.dev/)
* ðŸ§  [Ollama](https://ollama.com/) to run local LLMs (e.g., Mistral)
* ðŸ³ Optional Docker support
* ðŸ§¾ Disk logging for every interaction

---
## ðŸŽ¥ Demo

[Watch the MiniVault API demo video](https://screenrec.com/share/MqZacGF4Qh)

## âœ… Features

* `POST /generate` â†’ Takes a prompt, returns an LLM-generated response.
* Logs all prompts and responses to `./logs/interactions.log`.
* Uses Ollama locally, no internet/cloud LLMs involved.
* Clean modular structure.
* Optional: Command-line CLI.

---

## ðŸ“¦ Requirements

* Node.js `>=18`
* [Ollama](https://ollama.com/) installed and running locally
* Mistral model pulled (`ollama run mistral`)
* Optional: Docker Desktop (for containerized setup)

---

## ðŸ› ï¸ Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/sonip98/minivault-api.git
cd minivault-api
```

### 2. Install dependencies

```bash
npm install
```

### 3. Ensure Ollama is running locally

* [Install Ollama](https://ollama.com/download)
* Start Ollama:

```bash
ollama serve  # Or just `ollama run mistral` to auto-start server
```

* Pull the Mistral model:

```bash
ollama pull mistral
```

> Ollama will serve the model on `http://localhost:11434`

---

### 4. Start the API server

```bash
npm start
```

By default, the server runs on **`http://localhost:3000`**

---

## ðŸŽ¯ API Usage

### POST `/generate`

Send a prompt and get a response from the local model.

#### Request:

```bash
curl -X POST http://localhost:3000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is local-first AI?"}'
```

#### Response:

```json
{
  "response": "Local-first AI means that the artificial intelligence model runs on your local device rather than in the cloud, ensuring privacy and faster performance."
}
```

> ðŸ” Actual response varies depending on model and prompt.

---

## ðŸ“ File Structure

```
minivault-api/
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ generate.js        # POST /generate endpoint
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ ollama.js          # Ollama API integration
â”‚   â””â”€â”€ logger.js          # Log prompt/response to disk
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ prompts.log   # Request/response logs
â”œâ”€â”€ .env                 
â”œâ”€â”€ cli.js                 # (Optional) CLI interface
â”œâ”€â”€ Dockerfile             # Docker support
â”œâ”€â”€ index.js               # Fastify app entry point
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ðŸ³ Optional: Run via Docker

Build:

```bash
docker build -t minivault-api .
```

Run:

```bash
docker run -p 3000:3000 minivault-api
```

> âš ï¸ Ollama must be installed and running **on the host machine**.
> Inside Docker, the API uses `host.docker.internal:11434` to reach Ollama.

---

## âš™ï¸ Configuration

Create a `.env` file in the project root:

```env
OLLAMA_URL=http://localhost:11434
```

> If using Docker, change to:
> `OLLAMA_URL=http://host.docker.internal:11434`

## ðŸ”§ CLI Usage

Run a prompt from the terminal:

```bash
node cli.js "What is local-first AI?"
```

---

## ðŸ’¡ Tradeoffs / Design Notes

* Ollama runs as a separate service, enabling flexibility and modularity.
* Responses depend on actual LLM completions; no hardcoded stubs.
* Docker usage requires special handling (`host.docker.internal`) for Ollama access.

---

## ðŸ™Œ Thank You

This project demonstrates a real-world approach to local-first AI APIs.
Appreciate your time in reviewing!
